{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,.! \"\n",
    "target = \"Hello World!\"\n",
    "\n",
    "class Individual:\n",
    "     def __init__(self, string, fitness=0):\n",
    "        self.string = str(string)\n",
    "        self.fitness = fitness\n",
    "        \n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def choices(string, k):\n",
    "    s = ''\n",
    "    for i in range(5):\n",
    "        s = s+str(random.choice(alphabet))\n",
    "    return s\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def spawn_population(length=26, size=100): \n",
    "    pop = []\n",
    "    for i in range(size):\n",
    "        #string = ''.join(random.choices(alphabet,k=length))\n",
    "        string = ''.join(choices(alphabet,k=length))\n",
    "        individual = Individual(string)\n",
    "        pop.append(individual)\n",
    "        return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recombine(p1_, p2_): \n",
    "    p1 = p1_.string\n",
    "    p2 = p2_.string\n",
    "    child1 = []\n",
    "    child2 = []\n",
    "    cross_pt = random.randint(0,len(p1))\n",
    "    child1.extend(p1[0:cross_pt])\n",
    "    child1.extend(p2[cross_pt:])\n",
    "    child2.extend(p2[0:cross_pt])\n",
    "    child2.extend(p1[cross_pt:])\n",
    "    c1 = Individual(''.join(child1))\n",
    "    c2 = Individual(''.join(child2))\n",
    "    return c1, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutate(x, mut_rate=0.01): #B\n",
    "    new_x_ = []\n",
    "    for char in x.string:\n",
    "        if random.random() < mut_rate:\n",
    "            # new_x_.extend(random.choices(alphabet,k=1))\n",
    "            new_x_.extend(choices(alphabet,k=1))\n",
    "        else:\n",
    "            new_x_.append(char)\n",
    "    new_x = Individual(''.join(new_x_))\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_population(pop, target): \n",
    "    avg_fit = 0\n",
    "    for i in range(len(pop)):\n",
    "        fit = similar(str(pop[i]), target)\n",
    "        pop[i].fitness = fit\n",
    "        avg_fit += fit\n",
    "    avg_fit /= len(pop)\n",
    "    return pop, avg_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_generation(pop, size=100, length=26, mut_rate=0.01): \n",
    "    new_pop = []\n",
    "    while len(new_pop) < size:\n",
    "        # parents = random.choices(pop,k=2, weights=[x.fitness for x in pop])\n",
    "        parents = choices(pop,k=2)\n",
    "        offspring_ = recombine(parents[0],parents[1])\n",
    "        child1 = mutate(offspring_[0], mut_rate=mut_rate)\n",
    "        child2 = mutate(offspring_[1], mut_rate=mut_rate),\n",
    "        offspring = [child1, child2]\n",
    "        new_pop.extend(offspring)\n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-60c4bd58db80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpop_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnew_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutation_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-63ca21628367>\u001b[0m in \u001b[0;36mnext_generation\u001b[0;34m(pop, size, length, mut_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# parents = random.choices(pop,k=2, weights=[x.fitness for x in pop])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mparents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moffspring_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mchild1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmut_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mchild2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmut_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-280bd6f39286>\u001b[0m in \u001b[0;36mrecombine\u001b[0;34m(p1_, p2_)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp2_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchild1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchild2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "num_generations = 100\n",
    "population_size = 3000\n",
    "str_len = len(target)\n",
    "mutation_rate = 0.001\n",
    "pop_fit = []\n",
    "pop = spawn_population(size=population_size, length=str_len)\n",
    "for gen in range(num_generations):\n",
    "    pop, avg_fit = evaluate_population(pop, target)\n",
    "    pop_fit.append(avg_fit) \n",
    "    new_pop = next_generation(pop, size=population_size, length=str_len, mut_rate=mutation_rate)\n",
    "    pop = new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SimulatorState(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimulatorState, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.s_fc1 = torch.nn.Linear(512, 99)\n",
    "\n",
    "        self.action_fc1 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(100, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 16)\n",
    "\n",
    "        #         self.s_fc1 = torch.nn.Linear(50, 16)\n",
    "\n",
    "        self.reward_fc1 = torch.nn.Linear(50, 30)\n",
    "        self.reward_fc2 = torch.nn.Linear(30, 20)\n",
    "        self.reward_fc3 = torch.nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        state = x[:, :64]\n",
    "        a_x = x[:, 64:]\n",
    "\n",
    "        num_batch = state[0]\n",
    "\n",
    "        s_x = state.reshape(-1, 4, 4, 4)\n",
    "\n",
    "        s_x = F.relu(self.conv1(s_x))\n",
    "        s_x = F.relu(self.conv2(s_x))\n",
    "        s_x = F.relu(self.conv3(s_x))\n",
    "        s_x = s_x.view(-1, 8 * 64)\n",
    "        s_x = self.s_fc1(s_x)\n",
    "\n",
    "        a_x = self.action_fc1(a_x)\n",
    "\n",
    "        x = torch.cat((s_x, a_x), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        #         state_copy = state.reshape(-1,4,16).clone()\n",
    "        #         state_copy[np.arange(num_batch)][0][:] = 0\n",
    "        #         state_copy[np.arange(num_batch),s_x] = 1\n",
    "\n",
    "        #         r_x = F.relu(self.reward_fc1(state_copy))\n",
    "        #         r_x = F.relu(self.reward_fc2(r_x))\n",
    "        #         r_x = self.reward_fc3(r_x)\n",
    "\n",
    "        return F.softmax(x)\n",
    "\n",
    "\n",
    "class SimulatorReward(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimulatorReward, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(512, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 100)\n",
    "        self.fc3 = torch.nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 4, 4, 4)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.view(-1, 512)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tqdm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-dab0526e91ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mGridworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridworld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tqdm"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"script\")\n",
    "from Gridworld import Gridworld\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from simulator import SimulatorState, SimulatorReward\n",
    "from buffer import ExperienceReplay\n",
    "\n",
    "action_set = {\n",
    "    0: 'u',\n",
    "    1: 'd',\n",
    "    2: 'l',\n",
    "    3: 'r',\n",
    "}\n",
    "\n",
    "\n",
    "def running_mean(x, N=500):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "def encode_game_progress(reward):\n",
    "    if reward == 0:\n",
    "        return 0  # in progress\n",
    "    elif reward == 1:\n",
    "        return 1  # won\n",
    "    else:\n",
    "        return 2  # lost\n",
    "\n",
    "\n",
    "def decode_game_progress(index):\n",
    "    if index == 0:\n",
    "        return 0  # in progress\n",
    "    elif index == 1:\n",
    "        return 1  # won\n",
    "    else:\n",
    "        return -1  # lost\n",
    "\n",
    "\n",
    "def convert_to_state(state):\n",
    "    s_ = state.reshape(1, 4, 16)\n",
    "    s = s_.max(dim=2)\n",
    "    output = torch.zeroes(1, 4, 16)\n",
    "    output[0][s[0][0]] = 1\n",
    "    output[0][s[0][1]] = 1\n",
    "    output[0][s[0][2]] = 1\n",
    "    output[0][s[0][3]] = 1\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不足モジュール\n",
    "- tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ExperienceReplay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-5c77786aab39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloss_fn_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperienceReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ExperienceReplay' is not defined"
     ]
    }
   ],
   "source": [
    "epochs=5000\n",
    "mode='rand'\n",
    "warm_up_period=1000\n",
    "batch_size=1000\n",
    "max_steps=15\n",
    "lr = 0.0015\n",
    "device = torch.device('cpu')\n",
    "simulator_s = SimulatorState().to(device)\n",
    "simulator_r = SimulatorReward().to(device)\n",
    "opt_s = torch.optim.Adam(simulator_s.parameters(), lr=lr)\n",
    "opt_r = torch.optim.Adam(simulator_r.parameters(), lr=lr)\n",
    "\n",
    "loss_fn_state = torch.nn.CrossEntropyLoss()\n",
    "loss_fn_reward = torch.nn.CrossEntropyLoss(weight=torch.Tensor([1, 50, 50]))\n",
    "losses = []\n",
    "buffer = ExperienceReplay()\n",
    "\n",
    "progress = tqdm(range(epochs))\n",
    "for epoch_num in progress:\n",
    "    game = Gridworld(mode=mode)\n",
    "    z = 0\n",
    "    for step_num in args.max_steps:\n",
    "        # get starting state\n",
    "        state = torch.from_numpy(game.board.render_np()).float().reshape(64, )\n",
    "        # take random action\n",
    "        action_ = np.random.randint(0, 4)\n",
    "        action = action_set[action_]\n",
    "        action_vec = torch.zeros(4, )\n",
    "        action_vec[action_] = 1\n",
    "\n",
    "        game.makeMove(action)\n",
    "        next_state = torch.from_numpy(game.board.render_np()).float()\n",
    "        reward_ = encode_game_progress(game.reward())\n",
    "        buffer.add([(state, action_vec, next_state[0].argmax(), reward_, next_state)])\n",
    "\n",
    "        if len(buffer) >= args.warm_up_period:\n",
    "            minibatch = buffer.sample(args.batch_size)\n",
    "            opt_s.zero_grad()\n",
    "            opt_r.zero_grad()\n",
    "            states, actions, next_states_i, rewards, next_states = zip(*minibatch)\n",
    "            states = torch.stack(states).to(device)\n",
    "            actions = torch.stack(actions).to(device)\n",
    "            pred_states, _ = simulator_s(torch.cat((states, actions), dim=1)).to(device)\n",
    "            next_states_i = torch.stack(next_states_i).to(device)\n",
    "            pred_rewards = simulator_r(torch.stack(next_states)).to(device)\n",
    "            loss_state = loss_fn_state(pred_states, next_states_i)\n",
    "            loss_reward = loss_fn_reward(pred_rewards, torch.Tensor(rewards).long())\n",
    "            overall_loss = loss_state + loss_reward * 0.5\n",
    "            overall_loss.backward()\n",
    "            opt_r.step()\n",
    "            opt_s.step()\n",
    "            progress.set_description(\n",
    "                \"{:10.3f} {:10.3f}\".format(loss_state.detach().numpy(), loss_reward.detach().numpy()))\n",
    "            losses.append([loss_state.detach().numpy(), loss_reward.detach().numpy()])\n",
    "        if game.reward() in [1, -1]:\n",
    "            break\n",
    "losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
